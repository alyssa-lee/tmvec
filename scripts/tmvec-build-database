#!/usr/bin/env python3
import os
import numpy as np
import pandas as pd
import torch
from tm_vec.model import trans_basic_block, trans_basic_block_Config
from tm_vec.tm_vec_utils import encode
from tm_vec.utils import load_fasta_as_dict
from transformers import T5EncoderModel, T5Tokenizer
import gc
from pysam.libcfaidx import FastxFile
from pathlib import Path
import argparse


parser = argparse.ArgumentParser(description='Process TM-Vec arguments', add_help=True)
parser.add_argument("--input-fasta",
                    type=Path,
                    required=True,
                    help=("Input proteins in fasta format to "
                          "construct the database.")
)
parser.add_argument("--tm-vec-model",
                    type=Path,
                    required=True,
                    help="Model path for TM-Vec embedding model"
)
parser.add_argument("--tm-vec-config-path",
                    type=Path,
                    required=True,
                    help=("Config path for TM-Vec embedding model. "
                          "This is used to encode the proteins as "
                          "vectors to construct the database.")
)
parser.add_argument("--protrans-model",
                    type=Path,
                    default=None,
                    required=False,
                    help=("Model path for the ProTrans embedding model. "
                          "If this is not specified, then the model will "
                          "automatically be downloaded.")
)

parser.add_argument("--output",
                    type=Path,
                    required=True,
                    help="Output path for the database files"
)

parser.add_argument("--threads",
                    type=int,
                    default=1,
                    required=False,
                    help="Number of threads to use for parallel processing.")

# Load arguments
args = parser.parse_args()

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

if args.protrans_model is None:
    # Load the ProtTrans model and ProtTrans tokenizer
    tokenizer = T5Tokenizer.from_pretrained("Rostlab/prot_t5_xl_uniref50", do_lower_case=False)
    model = T5EncoderModel.from_pretrained("Rostlab/prot_t5_xl_uniref50")
else:
    tokenizer = T5Tokenizer.from_pretrained(args.protrans_model, do_lower_case=False)
    model = T5EncoderModel.from_pretrained(args.protrans_model)

gc.collect()
model = model.to(device)
model = model.eval()

# set up threads
torch.set_num_threads(args.threads)

# Load the Tm_Vec_Align TM model
tm_vec_model_config = trans_basic_block_Config.from_json(args.tm_vec_config_path)
model_deep = trans_basic_block.load_from_checkpoint(args.tm_vec_model,
                                                    config=tm_vec_model_config,
                                                    map_location=device)
model_deep = model_deep.to(device)
model_deep = model_deep.eval()
print("TM-Vec model loaded")


# Read in query sequences
records = load_fasta_as_dict(args.input_fasta)

# sort sequences by length
sorted_records = sorted(records.items(), key=lambda x: len(x[1]), reverse=True)
prefilter = len(sorted_records)
# remove sequences longer than 512
# limitation of the ProtT5-XL
sorted_records = {k: v for k, v in sorted_records if len(v) <= 512}
postfilter = len(sorted_records)
headers, flat_seqs = zip(*sorted_records.items())
print(f"Filtered {prefilter - postfilter} sequences longer than 512 aa; ProtT5-XL was only trained on these.")

# Embed all query sequences
encoded_database = encode(flat_seqs, model_deep, model, tokenizer, device)

# Metadata array
metdata = np.array(headers)

# Outputting results
os.mkdir(args.output)
path_output_database = os.path.join(args.output, 'db')
path_output_metadata = os.path.join(args.output, 'meta')
# Write out the embeddings
np.save(path_output_database, encoded_database)
# Write out metdata
np.save(path_output_metadata, metdata)
