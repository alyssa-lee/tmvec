#!/usr/bin/env python3
import numpy as np
import pandas as pd
import torch
from tm_vec.embed_structure_model import (trans_basic_block,
                                          trans_basic_block_Config)
from tm_vec.database import (format_ids, load_database, query)
from tm_vec.tm_vec_utils import encode

# from deepblast.dataset.utils import states2alignment
from transformers import T5EncoderModel, T5Tokenizer
import gc
from pathlib import Path
import argparse
from pysam.libcfaidx import FastaFile
from tm_vec.utils import load_fasta_as_dict

parser = argparse.ArgumentParser(
    description='Process TM-Vec arguments', add_help=True)

parser.add_argument("--query",
        type=Path,
        required=True,
        help="Input fasta-formatted data to query against database."
)

parser.add_argument("--database",
        type=Path,
        required=True,
        help="Database to query"
)

parser.add_argument("--database-fasta",
        type=Path,
        required=False,
        help=("Database that contains the corresponding "
              "protein sequences in fasta format.")
)

parser.add_argument("--metadata",
        type=Path,
        help="Metadata for queried database"
)

parser.add_argument("--tm-vec-model",
        type=Path,
        required=True,
        help="Model path for embedding"
)

parser.add_argument("--tm-vec-config",
        type=Path,
        required=True,
        help="Config path for embedding"
)

parser.add_argument("--deepblast-model",
        type=Path,
        help="DeepBLAST model path"
)

parser.add_argument("--protrans-model",
                    type=Path,
                    default=None,
                    required=False,
                    help=("Model path for the ProTrans embedding model. "
                          "If this is not specified, then the model will "
                          "automatically be downloaded.")
)
parser.add_argument("--device",
                    type=str,
                    default=None,
                    required=False,
                    help=(
                        "The device id to load the model onto. "
                        "This will specify whether or not a GPU "
                        "will be utilized.  If `gpu` is specified "
                        "then the first gpu device will be used."
                    )
)

parser.add_argument("--threads",
                    type=int,
                    default=1,
                    required=False,
                    help="Number of threads to use for parallel processing.")

parser.add_argument("--alignment-mode",
                    type=str,
                    default='needleman-wunsch',
                    required=False,
                    help=(
                        "`smith-waterman` or `needleman-wunch`."
                    )
)

parser.add_argument("--k-nearest-neighbors",
        type=int,
        default=5,
        help="Number of nearest neighbhors to return for each query."
)

parser.add_argument("--output-format",
                    type=str,
                    default='tabular',
                    required=False,
                    help=(
                        "Options include `tabular`, `alignment` "
                        "or `embedding`."
                    )
)

parser.add_argument("--output",
        type=Path,
        required=True,
        help="Output search results."
)

parser.add_argument("--output-embeddings",
        type=Path,
        default=None,
        required=False,
        help="Output encodings of query proteins (npy format)."
)

#Load arguments
args = parser.parse_args()

# Read in query sequences
records = load_fasta_as_dict(args.query)

# sort sequences by length
sorted_records = sorted(records.items(), key=lambda x: len(x[1]), reverse=True)
prefilter = len(sorted_records)
# remove sequences longer than 512
# limitation of the ProtT5-XL
sorted_records = {k: v for k, v in sorted_records if len(v) <= 512}
postfilter = len(sorted_records)
headers, flat_seqs = zip(*sorted_records.items())
print(f"Filtered {prefilter - postfilter} sequences longer than 512 aa; ProtT5-XL was only trained on these.")

# Load metadata if it exists
metadata_database = np.load(args.metadata)

# Set device
if args.device == 'cpu':
    device = torch.device('cpu')
elif torch.cuda.is_available() and args.device is not None:
    if args.device == 'gpu':
        device = torch.device(f'cuda:0')
    else:
        device = torch.device(f'cuda:{int(args.device)}')
else:
    print('Models will be loaded on CPU.')
    device = torch.device('cpu')

if args.protrans_model is None:
    # Load the ProtTrans model and ProtTrans tokenizer
    tokenizer = T5Tokenizer.from_pretrained("Rostlab/prot_t5_xl_uniref50",
                                            do_lower_case=False,
                                            legacy=True)
    model = T5EncoderModel.from_pretrained("Rostlab/prot_t5_xl_uniref50")
else:
    tokenizer = T5Tokenizer.from_pretrained(args.protrans_model,
                                            do_lower_case=False,
                                            legacy=True)
    model = T5EncoderModel.from_pretrained(args.protrans_model)

gc.collect()
model = model.to(device)
model = model.eval()

# set threads
torch.set_num_threads(args.threads)

print("ProtTrans model downloaded")


# Load the Tm_Vec_Align TM model
tm_vec_model_config = trans_basic_block_Config.from_json(args.tm_vec_config)
model_deep = trans_basic_block.load_from_checkpoint(
    args.tm_vec_model, config=tm_vec_model_config,
    map_location=device)

model_deep = model_deep.to(device)
model_deep = model_deep.eval()
print("TM-Vec model loaded")

# Read in query database
query_database = np.load(args.database)

# Embed all query sequences
# Build query array and save the embeddings (will write them to output)
queries = encode(flat_seqs, model_deep, model, tokenizer, device)

# Build an indexed database
index = load_database(args.database)

# Return the nearest neighbors
k = args.k_nearest_neighbors
D, I = query(index, queries, k)

# Return the metadata for the nearest neighbor results
near_ids = []

print("meta data loaded")

for i in range(I.shape[0]):
    meta = metadata_database[I[i]]
    near_ids.append(list(meta))

near_ids = np.array(near_ids)

del model_deep  # need to clear space for DeepBLAST aligner

# # Alignment section
# # If we are also aligning proteins, load tm_vec align and
# # align nearest neighbors if true
# if args.deepblast_model is not None and args.output_format == 'alignment':
#     # moving imports here because it may take a very long time
#     from deepblast.dataset.utils import states2alignment
#     from deepblast.utils import load_model

#     align_model = load_model(
#         args.deepblast_model, lm=model, tokenizer=tokenizer,
#         alignment_mode=args.alignment_mode,
#         device=device)

#     align_model = align_model.to(device)
#     seq_db = FastaFile(args.database_fasta)

#     alignments = []
#     with open(args.output, 'w') as fh:
#         for i in range(I.shape[0]):
#             for j in range(I.shape[1]):
#                 x = flat_seqs[i]
#                 seq_i = metadata_database[I[i, j]]
#                 y = seq_db.fetch(seq_i)
#                 try:
#                     pred_alignment = align_model.align(x, y)
#                     # Note : there is an edge case that smith-waterman will throw errors,
#                     # but needleman-wunsch won't.
#                     x_aligned, y_aligned = states2alignment(pred_alignment, x, y)
#                     alignments_i = [x_aligned, pred_alignment, y_aligned]
#                     # sWrite out the alignments
#                     x, s, y = alignments_i
#                     # TODO : not clear how to get the sequence IDS
#                     ix, iy = format_ids(headers[i], seq_i)
#                     fh.write(ix + ' ' + x + '\n')
#                     fh.write(iy + ' ' + y + '\n\n')
#                 except:
#                     print(f'No valid alignments found for {headers[i]} {seq_i}')


# Outputting results
# Write out the nearest neighbors
# (if no database meta data is providedwrite out their database indices)
if args.output_format == 'tabular':
    nids = pd.DataFrame(near_ids, index=headers)
    nids.index.name = 'query_id'
    nids = pd.melt(nids.reset_index(), id_vars='query_id', var_name='rank', value_name='database_id')
    # correct ranks to start from 1
    nids['rank'] = nids['rank'] + 1

    tms = pd.DataFrame(D, index=headers)
    tms = pd.melt(tms, var_name='query_id', value_name='tm-score')
    nids = pd.concat((nids, tms[['tm-score']]), axis=1)
    nids = nids.sort_values(['query_id', 'rank'])
    nids.to_csv(args.output, sep='\t', index=None)


# Write out the embeddings
if args.output_embeddings is not None:
    np.save(args.output_embeddings, queries)
